{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import json\n",
    " \n",
    "x_text = []\n",
    "y = []\n",
    "with open('reviews_Musical_Instruments_5.json\\Musical_Instruments_5.json') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        x_text.append(data['reviewText'].lower().strip())\n",
    "        y.append(int(data['overall']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_text = x_text[:1000]\n",
    "y = y[:1000]\n",
    "train_end_idx=int(0.9 * len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f866dd9918a27f9ba2f31ef62a061fa1",
     "grade": false,
     "grade_id": "cell-87edb8b6e5279a0e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "w2i = defaultdict(lambda: len(w2i))\n",
    "UNK = w2i[\"<unk>\"] \n",
    "\n",
    "\n",
    "t = []\n",
    "for x in x_text:\n",
    "    t += x.split(' ')\n",
    "\n",
    "for w, n in Counter(t).items():\n",
    "    if n > 5:\n",
    "        w2i[w]\n",
    "n_words = len(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b1000affcd1be506cad1e3a22a63cf1",
     "grade": true,
     "grade_id": "cell-59701c5db1041735",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = defaultdict(lambda: UNK, w2i) \n",
    "class2i = defaultdict(lambda: len(class2i))\n",
    "    \n",
    "def read_dataset(start_idx,end_idx):\n",
    "    for i, text in enumerate(x_text[start_idx:end_idx]):\n",
    "        yield ([w2i[x] for x in text.split(\" \")], class2i[y[i]])\n",
    "        \n",
    "train = list(read_dataset(0, train_end_idx))\n",
    "dev = list(read_dataset(train_end_idx, len(y)))\n",
    "n_class = len(class2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1389 5\n"
     ]
    }
   ],
   "source": [
    "print(n_words, n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "134cf7cd8660133b178caff1ca2fcdfc",
     "grade": false,
     "grade_id": "cell-9ba79ce47d0c6a5e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 20\n",
    "C = torch.rand((n_words, EMBEDDING_SIZE), requires_grad=True)\n",
    "W = torch.rand((n_class, EMBEDDING_SIZE+1), requires_grad=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(torch.nn.Module):\n",
    "    def __init__(self, n_words, emb_size, n_class):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(n_words, emb_size)\n",
    "        self.linear = torch.nn.Linear(in_features=emb_size, out_features=n_class, bias=True)\n",
    "        torch.nn.init.uniform_(self.embedding.weight, -0.25, 0.25)\n",
    "        torch.nn.init.xavier_uniform_(self.linear.weight)\n",
    "\n",
    "    def forward(self, words):\n",
    "        emb = self.embedding(words)                 \n",
    "        h = emb.mean(dim=0)                         \n",
    "        h = torch.reshape(h, (1,-1))\n",
    "        out = self.linear(h)              \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.0006,  0.0624,  0.0098,  ...,  0.1398, -0.0790,  0.2460],\n",
      "        [-0.1673,  0.2464, -0.0037,  ..., -0.2457, -0.0821,  0.2042],\n",
      "        [-0.2050,  0.1472,  0.0635,  ...,  0.1149,  0.1142,  0.0938],\n",
      "        ...,\n",
      "        [ 0.1722, -0.1179, -0.1573,  ...,  0.2258,  0.0708, -0.1116],\n",
      "        [-0.0057,  0.1040,  0.1037,  ...,  0.1357, -0.1066, -0.0345],\n",
      "        [-0.0880, -0.0358,  0.1797,  ..., -0.1048, -0.0637,  0.1526]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.3197, -0.1474, -0.1573,  0.1251, -0.0480,  0.4300,  0.1106,  0.1935,\n",
      "         -0.3719,  0.0290,  0.0591, -0.4199, -0.1380, -0.1446,  0.1038,  0.4848,\n",
      "         -0.3397, -0.0840, -0.1847,  0.0368],\n",
      "        [-0.4550,  0.3454,  0.0437, -0.2554, -0.4364,  0.2466,  0.0495, -0.0443,\n",
      "         -0.3265,  0.4659, -0.4587,  0.4723,  0.3461,  0.2374,  0.1582,  0.4053,\n",
      "          0.4277, -0.0298,  0.4010,  0.2092],\n",
      "        [ 0.2309,  0.3550,  0.1285,  0.0266,  0.2219,  0.1832,  0.1480, -0.3874,\n",
      "          0.0611,  0.4012, -0.2741, -0.1606, -0.2473, -0.1751, -0.3324,  0.3981,\n",
      "          0.0131, -0.4140,  0.0766, -0.0350],\n",
      "        [-0.2677, -0.3426,  0.2788,  0.0790, -0.3280, -0.0958, -0.3890,  0.2225,\n",
      "         -0.0438,  0.3428, -0.3099,  0.4113, -0.3962, -0.2583,  0.1075, -0.1129,\n",
      "          0.1426,  0.1054,  0.0196, -0.2052],\n",
      "        [ 0.1090,  0.1786, -0.3608,  0.0155,  0.2139,  0.3226,  0.3375, -0.2933,\n",
      "          0.0815,  0.2080,  0.2688, -0.0487,  0.3940,  0.1775, -0.2095, -0.2158,\n",
      "         -0.4564, -0.0400, -0.4651,  0.0795]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.1137,  0.0356,  0.1859, -0.0046, -0.0806], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "model = SimpleModel(n_words, EMBEDDING_SIZE, n_class)\n",
    "print([i for i in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5f2f3a6cb7d7b2d2e34e63a2b54004b",
     "grade": false,
     "grade_id": "cell-87d2dfc0c801725a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: avg. train loss=0.8431\n",
      "iter 1: avg. train loss=0.8105\n",
      "iter 2: avg. train loss=0.7889\n",
      "iter 3: avg. train loss=0.7576\n",
      "iter 4: avg. train loss=0.7366\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "eta = 0.5  # prędkość uczenia\n",
    "\n",
    "for i in range(epochs):\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    for words, tag in train:\n",
    "        pred = model(torch.tensor(words))\n",
    "        loss = F.cross_entropy(pred.reshape(1,-1), torch.tensor(tag).reshape(1))\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for p in model.parameters():\n",
    "                p -= eta * p.grad\n",
    "            model.zero_grad()\n",
    "        train_loss += loss\n",
    "    print(\"iter %r: avg. train loss=%.4f\" % (i, train_loss / len(train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92b92a0626a6c533b3b61aa2e6c18ddd",
     "grade": false,
     "grade_id": "cell-5588a40beb11d256",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: avg. train loss=0.6740\n",
      "iter 1: avg. train loss=0.6522\n",
      "iter 2: avg. train loss=0.6149\n",
      "iter 3: avg. train loss=0.6004\n",
      "iter 4: avg. train loss=0.5691\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "epochs = 5\n",
    "eta = 0.5  \n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=eta)\n",
    "\n",
    "for i in range(epochs):\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    for words, tag in train:\n",
    "        pred = model(torch.tensor(words))\n",
    "        loss = F.cross_entropy(pred.view(1,-1), torch.tensor([tag]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss += loss\n",
    "    print(\"iter %r: avg. train loss=%.4f\" % (i, train_loss / len(train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8e23fa15d0164d9a5260ab5ea518263",
     "grade": false,
     "grade_id": "cell-dbb714252ae2ebc7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: avg. train loss=0.9462\n",
      "iter 1: avg. train loss=0.9245\n",
      "iter 2: avg. train loss=0.9037\n",
      "iter 3: avg. train loss=0.9051\n",
      "iter 4: avg. train loss=0.8956\n",
      "iter 5: avg. train loss=0.8679\n",
      "iter 6: avg. train loss=0.8446\n",
      "iter 7: avg. train loss=0.7814\n",
      "iter 8: avg. train loss=0.7039\n",
      "iter 9: avg. train loss=0.5822\n",
      "iter 10: avg. train loss=0.4217\n",
      "iter 11: avg. train loss=0.2830\n",
      "iter 12: avg. train loss=0.1830\n",
      "iter 13: avg. train loss=0.1127\n",
      "iter 14: avg. train loss=0.0738\n",
      "iter 15: avg. train loss=0.0477\n",
      "iter 16: avg. train loss=0.0319\n",
      "iter 17: avg. train loss=0.0221\n",
      "iter 18: avg. train loss=0.0160\n",
      "iter 19: avg. train loss=0.0118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (emb): Embedding(1389, 20)\n",
       "  (conv): Conv1d(20, 64, kernel_size=(2,), stride=(1,))\n",
       "  (lin): Linear(in_features=64, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, n_words, emb_size, num_filters, window_size, ntags):\n",
    "        super().__init__()\n",
    "        self.emb_size, self.num_filters = emb_size, num_filters\n",
    "        self.emb = torch.nn.Embedding(n_words, emb_size)\n",
    "        self.conv = torch.nn.Conv1d(emb_size, num_filters, kernel_size=window_size)\n",
    "        self.lin = torch.nn.Linear(in_features=num_filters, out_features=ntags, bias=True)\n",
    "        for layer in [self.emb, self.conv, self.lin]:\n",
    "            torch.nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    def forward(self, words):\n",
    "        x = words\n",
    "        x = self.emb(x).reshape(1, self.emb_size, -1)  \n",
    "        x = self.conv(x).reshape(self.num_filters, -1) \n",
    "        x = F.relu(x).max(1).values                 \n",
    "        x = self.lin(x.reshape(-1))                    \n",
    "        return x\n",
    "       \n",
    "def train_loop(model, epochs=5, lr=0.05):\n",
    "    optim = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    for i in range(epochs):\n",
    "        random.shuffle(train)\n",
    "        train_loss = 0.0\n",
    "        for words, tag in train:\n",
    "            pred = model(torch.tensor(words))\n",
    "            loss = F.cross_entropy(pred.view(1,-1), torch.tensor([tag]))\n",
    "            loss.backward()\n",
    "            train_loss += loss\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "        print(\"iter %r: avg. train loss=%.4f\" % (i, train_loss / len(train)))\n",
    "    return model\n",
    "        \n",
    "model = CNN(n_words=n_words, emb_size=EMBEDDING_SIZE, ntags=n_class, num_filters=64, window_size=2)\n",
    "train_loop(model, epochs=20, lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
